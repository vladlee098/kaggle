{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"**                              Identify the number of channels open at each time point**\n\nMany diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n\nThe University of Liverpool’s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you’ll use ion channel data to better model automatic identification methods. If successful, you’ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\n\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import class_weight\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error, f1_score, confusion_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\n\n\nfrom tensorflow.keras.models import Sequential, Model\n\n#from tensorflow.keras.layers import InputLayer\nfrom tensorflow.keras.layers import LSTM, Bidirectional, add, concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv2DTranspose, AveragePooling1D, UpSampling1D\nfrom tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation, TimeDistributed\nfrom tensorflow.keras.layers import Multiply, Add, Concatenate, Flatten, Average, Lambda\n\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, Callback, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.constraints import unit_norm, max_norm\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\n#from tensorflow.keras.utils import np_utils\n\nfrom tensorflow.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## utils","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024 ** 2 # just added \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage(deep=True).sum() / 1024 ** 2\n    percent = 100 * (start_mem - end_mem) / start_mem\n    print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, percent))\n    return df\n\ndef get_stats(df):\n    stats = pd.DataFrame(index=df.columns, columns=['na_count', 'n_unique', 'type', 'memory_usage'])\n    for col in df.columns:\n        stats.loc[col] = [df[col].isna().sum(), df[col].nunique(dropna=False), df[col].dtypes, df[col].memory_usage(deep=True, index=False) / 1024**2]\n    stats.loc['Overall'] = [stats['na_count'].sum(), stats['n_unique'].sum(), None, df.memory_usage(deep=True).sum() / 1024**2]\n    return stats\n\ndef print_header():\n    print('col         conversion        dtype    na    uniq  size')\n    print()\n    \ndef print_values(name, conversion, col):\n    template = '{:10}  {:16}  {:>7}  {:2}  {:6}  {:1.2f}MB'\n    print(template.format(name, conversion, str(col.dtypes), col.isna().sum(), col.nunique(dropna=False), col.memory_usage(deep=True, index=False) / 1024 ** 2))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras.backend as K\n\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def display_set(df, column, n_sample, figsize ):\n    f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = figsize )\n    sns.lineplot(x= df.index[::n_sample], y = df[column][::n_sample], ax=ax1)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix\n# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load train and test datasets"},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch).\nIn other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/liverpool-ion-switching/'\n\n\ndef read_and_clean(file_name):\n    return pd.read_csv(PATH + file_name)\n\n\ntrain = read_and_clean('train.csv')\ntest = read_and_clean('test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merge train and test and feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Most of the ideas have been taken from https://www.kaggle.com/gpreda/ion-switching-advanced-eda-and-prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['train'] = True\ntest['train'] = False\ntest['open_channels'] = np.nan\n\nfull = pd.concat([train, test], sort=False).reset_index(drop=True)\nfull['train'] = full['train'].astype('bool')\nfull = full.sort_values(by=['time']).reset_index(drop=True)\n\nfull.index = ((full.time * 10_000) - 1).values\nfull['batch'] = full.index // 50_000\nfull['batch_index'] = full.batch // 50_000\nfull['batch_index'] = full.index  - (full.batch * 50_000)\nfull['batch_slices'] = full['batch_index']  // 5_000\nfull['batch_slices2'] = full.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n\n# 50_000 Batch Features\nfull['signal_batch_min'] = full.groupby('batch')['signal'].transform('min')\nfull['signal_batch_max'] = full.groupby('batch')['signal'].transform('max')\nfull['signal_batch_std'] = full.groupby('batch')['signal'].transform('std')\nfull['signal_batch_mean'] = full.groupby('batch')['signal'].transform('mean')\nfull['signal_batch_median'] = full.groupby('batch')['signal'].transform('median')\n#full['signal_batch_skew'] = full.groupby('batch')['signal'].transform('skew')\n#full['mean_abs_chg_batch'] = full.groupby(['batch'])['signal'].transform(lambda x: np.mean(np.abs(np.diff(x))))\n#full['median_abs_chg_batch'] = full.groupby(['batch'])['signal'].transform(lambda x: np.median(np.abs(np.diff(x))))\n#full['abs_max_batch'] = full.groupby(['batch'])['signal'].transform(lambda x: np.max(np.abs(x)))\n#full['abs_min_batch'] = full.groupby(['batch'])['signal'].transform(lambda x: np.min(np.abs(x)))\n#full['abs_mean_batch'] = full.groupby(['batch'])['signal'].transform(lambda x: np.mean(np.abs(x)))\n#full['abs_median_batch'] = full.groupby(['batch'])['signal'].transform(lambda x: np.median(np.abs(x)))\n#full['moving_average_batch_1000_mean'] = full.groupby(['batch'])['signal'].rolling(window=1000).mean().mean(skipna=True)\n\n#full['range_batch'] = full['signal_batch_max'] - full['signal_batch_min']\n#full['maxtomin_batch'] = full['signal_batch_max'] / full['signal_batch_min']\n#full['abs_avg_batch'] = (full['abs_min_batch'] + full['abs_max_batch']) / 2\n\n#add shifts\nfull['signal_shift+1'] = full.groupby(['batch']).shift(1)['signal']\nfull['signal_shift-1'] = full.groupby(['batch']).shift(-1)['signal']\n#full['signal_shift+2'] = full.groupby(['batch']).shift(2)['signal']\n#full['signal_shift-2'] = full.groupby(['batch']).shift(-2)['signal']\n\n#full['signal_shift+1_5k'] = full.groupby(['batch_slices2']).shift(1)['signal']\n#full['signal_shift-1_5k'] = full.groupby(['batch_slices2']).shift(-1)['signal']\n#full['signal_shift+2_5k'] = full.groupby(['batch_slices2']).shift(2)['signal']\n#full['signal_shift-2_5k'] = full.groupby(['batch_slices2']).shift(-2)['signal']\n\n#full['abs_max_signal_shift+1_5k'] = full['signal_shift+1_5k'].transform(lambda x: np.max(np.abs(x)))\n#full['abs_max_signal_shift-1_5k'] = full['signal_shift-1_5k'].transform(lambda x: np.max(np.abs(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_sizes = [10, 50, 500, 1000, 5000]\nfor window in window_sizes:\n    full[\"rolling_mean_\" + str(window)] = full['signal'].rolling(window=window).mean()\n    full[\"rolling_std_\" + str(window)] = full['signal'].rolling(window=window).std()\n    full[\"rolling_var_\" + str(window)] = full['signal'].rolling(window=window).var()\n    full[\"rolling_min_\" + str(window)] = full['signal'].rolling(window=window).min()\n    full[\"rolling_max_\" + str(window)] = full['signal'].rolling(window=window).max()\n    \n    full[\"rolling_min_max_ratio_\" + str(window)] = full[\"rolling_min_\" + str(window)] / full[\"rolling_max_\" + str(window)]\n    full[\"rolling_min_max_diff_\" + str(window)] = full[\"rolling_max_\" + str(window)] - full[\"rolling_min_\" + str(window)]\n    \n    a = (full['signal'] - full['rolling_min_' + str(window)]) / (full['rolling_max_' + str(window)] - full['rolling_min_' + str(window)])\n    full[\"norm_\" + str(window)] = a * (np.floor(full['rolling_max_' + str(window)]) - np.ceil(full['rolling_min_' + str(window)]))\n    \nfull = full.replace([np.inf, -np.inf], np.nan)    \nfull.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FEATURES = [f for f in full.columns if f not in ['open_channels','index','time','train', 'batch_slices', 'batch_slices2']]\nprint(f\"Features: {len(FEATURES)}\")\nprint([f for f in FEATURES])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_stats(full)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display train and test signals"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_BATCH_SIZE = 500000\n\nTRAIN_SAMPLE_RATE = 100\nTRAIN_BATCH_SIZE = int(len(train)/TRAIN_SAMPLE_RATE)\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (20,4))\nsns.lineplot(data=train.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nsns.lineplot(data=train.open_channels[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full train signal')\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (10,4))\nsns.lineplot(data=test.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full test signal')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## By batch"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    XX = train.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    yy = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[int(i/5), i%5], hue=\"size\", size=\"size\")\n    sns.scatterplot(data=yy[::TRAIN_SAMPLE_RATE], ax=axes[int(i/5), i%5], hue=\"size\", size=\"size\")\n    axes[int(i/5), i%5].set_title(f'Train Batch# {i+1}')\n    \nf, axes = plt.subplots(nrows = 1, ncols = 5, figsize = (26,6))\nfor i in range(4):\n    XX = test.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[i], hue=\"size\", size=\"size\")\n    axes[i].set_title(f'Test Batch# {i+1}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Open channels value count"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 6))\nsns.countplot(x=\"open_channels\", data=train, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Distributions for open channels for each batch"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n\nf, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    y = pd.DataFrame()\n    sns.countplot( x = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1], ax=axes[int(i/5), i%5])\n    axes[int(i/5), i%5].set_title(f'Train Batch# {i+1}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compute correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = full[::1000].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = full.query('train').copy()\ntest_df = full.query('not train').copy()\ntrain_df['open_channels'] = train_df['open_channels'].astype(int)\n\nX_all_train = train_df[FEATURES]\nX_all_test = test_df[FEATURES]\ny_all_train = train_df['open_channels'].values\n\n## reduce amount of data to speed things up\nX_train = X_all_train[::10]\ny_train = y_all_train[::10]\nX_test = X_all_test[::10] ## this is just for display purposes\n\n\ndel full\ndel train_df\ndel test_df\ngc.collect()\n\nprint(f'Original sizes: X_all_train: {X_all_train.shape}, y_all_train: {y_all_train.shape}, X_all_test: {X_all_test.shape}' )\nprint(f'Reduced train sizes: X_train: {X_train.shape}, y_train: {y_all_train.shape}, X_test: {X_test.shape}' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model(input_shape, units = 32, max_channels = 11, optimizer='adam'):\n    model = Sequential()\n    model.add(LSTM(units, input_shape=(input_shape[1], input_shape[2]), return_sequences=True))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(LSTM(units))\n    model.add(Dense(units))\n    model.add(Dense(max_channels, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', f1])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## it seems LSTM model will not converge with all generated features (at least for me :) \n## so, select some that work\n\nRANDOM_SEED = 42\n\nLSTM_FEATURES = ['signal', 'rolling_min_max_diff_5000', 'signal_batch_mean', 'norm_5000', 'batch_index', \n                 'rolling_max_5000', 'norm_1000', 'signal_shift-1', 'signal_shift+1']\n\nX = X_train[LSTM_FEATURES]\ny = y_train\n\nX = X[::10]\ny = y[::10]\n\n\n## \n#X.signal = (X.signal-X.signal.mean())/X.signal.std()\n\n## reshape for LSTM\nX = X.values.reshape(-1,len(LSTM_FEATURES),1)\n## using categorical_crossentropy\nyy = to_categorical(y, num_classes=11)\n\ntrain_idx, val_idx = train_test_split(np.arange(X.shape[0]), random_state = RANDOM_SEED, test_size = 0.2)\n\nX_t = X[train_idx] \ny_t = yy[train_idx] \nX_v = X[val_idx]\ny_v = yy[val_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64\nEPOCHS = 20\n\nes = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)\nlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.000001, verbose=1)\n\nadam = Adam(0.01)\n\nmodel = baseline_model(X_t.shape, optimizer=adam)\nhistory = model.fit( X_t, y_t, validation_data=(X_v, y_v), callbacks=[es,lr],\n                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n                    shuffle=False, workers=8, use_multiprocessing=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (16,8))\nsns.lineplot(data=np.asarray(history.history['loss']), label='loss', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['val_loss']), label='val_loss', ax=ax1)\n#sns.lineplot(data=np.asarray(history.history['acc']), label='acc', ax=ax1)\n#sns.lineplot(data=np.asarray(history.history['val_acc']), label='val_acc', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['f1']), label='f1', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['val_f1']), label='val_f1', ax=ax1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(model.predict(X_v), axis=1).reshape(-1)\nyhat = y[val_idx]\n\nprint(\"SCORE_oldmetric: \", cohen_kappa_score(yhat, y_pred, weights=\"quadratic\"))\nprint(\"SCORE_newmetric: \", f1_score(yhat, y_pred, average=\"macro\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(model.predict(X_all_test[LSTM_FEATURES].values.reshape(-1,len(LSTM_FEATURES), 1)), axis=1).reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\nsub.open_channels = np.array(np.round(y_pred,0), np.int)\nsub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head(25)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}