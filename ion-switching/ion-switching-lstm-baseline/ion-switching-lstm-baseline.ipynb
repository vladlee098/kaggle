{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Thanks\n\n\nhttps://www.kaggle.com/hirayukis/lightgbm-keras-and-4-kfold\n\nhttps://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\n\nhttps://www.kaggle.com/vbmokin/ion-switching-advanced-fe-lgb-xgb-confmatrix\n\nhttps://www.kaggle.com/martxelo/fe-and-ensemble-mlp-and-lgbm\n\nhttps://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr\n\nhttps://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\n\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport joblib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import class_weight\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error, f1_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef get_stats(df):\n    stats = pd.DataFrame(index=df.columns, columns=['na_count', 'n_unique', 'type', 'memory_usage'])\n    for col in df.columns:\n        stats.loc[col] = [df[col].isna().sum(), df[col].nunique(dropna=False), df[col].dtypes, df[col].memory_usage(deep=True, index=False) / 1024**2]\n    stats.loc['Overall'] = [stats['na_count'].sum(), stats['n_unique'].sum(), None, df.memory_usage(deep=True).sum() / 1024**2]\n    return stats\n\ndef print_header():\n    print('col         conversion        dtype    na    uniq  size')\n    print()\n    \ndef print_values(name, conversion, col):\n    template = '{:10}  {:16}  {:>7}  {:2}  {:6}  {:1.2f}MB'\n    print(template.format(name, conversion, str(col.dtypes), col.isna().sum(), col.nunique(dropna=False), col.memory_usage(deep=True, index=False) / 1024 ** 2))\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    #tf.random.set_seed(seed)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def display_set(df, column, n_sample, figsize ):\n    f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = figsize )\n    sns.lineplot(x= df.index[::n_sample], y = df[column][::n_sample], ax=ax1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_class_weight(classes, exp=1):\n    '''\n    Weight of the class is inversely proportional to the population of the class.\n    There is an exponent for adding more weight.\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()/np.power(hist, exp)\n    \n    return class_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/siavrez/simple-eda-model\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)\n\ndef multiclass_F1_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    num_labels = 11\n    preds = preds.reshape(num_labels, len(preds)//num_labels)\n    preds = np.argmax(preds, axis=0)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('MacroF1Metric', score, True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize F1 (Macro) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _f1_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        return -f1_score(y, X_p, average = 'macro')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._f1_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']\n    \n    \ndef optimize_predictions(prediction, coefficients):\n    prediction[prediction <= coefficients[0]] = 0\n    prediction[np.where(np.logical_and(prediction > coefficients[0], prediction <= coefficients[1]))] = 1\n    prediction[np.where(np.logical_and(prediction > coefficients[1], prediction <= coefficients[2]))] = 2\n    prediction[np.where(np.logical_and(prediction > coefficients[2], prediction <= coefficients[3]))] = 3\n    prediction[np.where(np.logical_and(prediction > coefficients[3], prediction <= coefficients[4]))] = 4\n    prediction[np.where(np.logical_and(prediction > coefficients[4], prediction <= coefficients[5]))] = 5\n    prediction[np.where(np.logical_and(prediction > coefficients[5], prediction <= coefficients[6]))] = 6\n    prediction[np.where(np.logical_and(prediction > coefficients[6], prediction <= coefficients[7]))] = 7\n    prediction[np.where(np.logical_and(prediction > coefficients[7], prediction <= coefficients[8]))] = 8\n    prediction[np.where(np.logical_and(prediction > coefficients[8], prediction <= coefficients[9]))] = 9\n    prediction[prediction > coefficients[9]] = 10\n    \n    return prediction    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load train and test datasets"},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42\nGROUP_BATCH_SIZE2 = 80000\nGROUP_BATCH_SIZE1 = 20000\nWINDOW_SIZES = [3, 5, 10, 50, 100, 500, 1000, 5000]\n\n## 3, 5, 2000, 5000\n\nseed_everything(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/data-without-drift/'\n#PATH = '/kaggle/input/liverpool-ion-switching/'\n\ntrain = pd.read_csv(PATH + 'train_clean.csv')\ntest = pd.read_csv(PATH + 'test_clean.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def add_category(train, test):\n    train[\"category\"] = 0\n    test[\"category\"] = 0\n    \n    # train segments with more then 9 open channels classes\n    train.loc[2_000_000:2_500_000-1, 'category'] = 1\n    train.loc[4_500_000:5_000_000-1, 'category'] = 1\n    \n    # test segments with more then 9 open channels classes (potentially)\n    test.loc[500_000:600_000-1, \"category\"] = 1\n    test.loc[700_000:800_000-1, \"category\"] = 1\n    \n    return train, test\n\n#train, test = add_category(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# create batches of GROUP_BATCH_SIZE observations\ndef batching(df, batch_size, gr_name='group'):\n    df[gr_name] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df[gr_name] = df[gr_name].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.values.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n    return train, test\n\ndef run_feat_engineering(df, batch_size, gr_name='group'):\n    df = batching(df, batch_size = batch_size, gr_name=gr_name)\n    df['signal_2'] = df['signal'] ** 2\n    df['signal_2-7500-mean'] = df['signal_2'] - df['signal_2'].rolling(window=7500).mean()    \n    #df['signal_2-mean'] = df['signal_2'] - df['signal_2'].mean()\n    return df\n\n#train, test = normalize(train, test)\ntrain = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE1, gr_name='group1')\ntrain = batching(train, batch_size = GROUP_BATCH_SIZE2, gr_name='group2')\n\ntest = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE1, gr_name='group1')\ntest = batching(test, batch_size = GROUP_BATCH_SIZE2, gr_name='group2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n## add some noise\n\nSTD = 0.01\n\nold_data = train['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(train)) \ntrain['signal'] = new_data\n\nold_data = test['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(test)) \ntest['signal'] = new_data\n\ndel old_data, new_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef gen_roll_features(full, win_sizes = WINDOW_SIZES):\n    for window in tqdm(win_sizes):\n        full[\"rolling_mean_\" + str(window)] = full['signal'].rolling(window=window).mean()\n        full[\"rolling_std_\" + str(window)] = full['signal'].rolling(window=window).std()\n        full[\"rolling_var_\" + str(window)] = full['signal'].rolling(window=window).var()\n        full[\"rolling_min_\" + str(window)] = full['signal'].rolling(window=window).min()\n        full[\"rolling_max_\" + str(window)] = full['signal'].rolling(window=window).max()\n\n        a = (full['signal'] - full['rolling_min_' + str(window)]) / (full['rolling_max_' + str(window)] - full['rolling_min_' + str(window)])\n        full[\"norm_\" + str(window)] = a * (np.floor(full['rolling_max_' + str(window)]) - np.ceil(full['rolling_min_' + str(window)]))\n    return full\n\ntrain = gen_roll_features(train)\ntest = gen_roll_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef gen_sig_features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  // 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in tqdm(['batch','batch_slices2']):\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] / d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_sig_features(train)\ntest = gen_sig_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef gen_shift_features(df):\n    # add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan\n    \n    df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'], inplace=True)\n    gc.collect()\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group1', 'group2', 'category', 'index']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_shift_features(train)\ntest = gen_shift_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BEST_FEATURES = [\n    'signal',\n    'signal_2', \n    'signal_2-7500-mean',\n    'minbatch_slices2_msignal',\n    'rolling_max_1000_msignal',\n    'signal_shift_-2_msignal',\n    'rangebatch_msignal',\n    'medianbatch_slices2_msignal',\n    'signal_shift_-1_msignal',\n    'mean_abs_chgbatch_slices2',\n    'abs_avgbatch_msignal',\n    'signal_shift_+1_msignal',\n    'rolling_min_10_msignal',\n    'abs_maxbatch_msignal',\n    'minbatch_msignal',\n    'mean_abs_chgbatch',\n    'abs_minbatch',\n    'signal_2_msignal',\n    'maxbatch_slices2_msignal',\n    'rolling_var_1000_msignal',\n    'rolling_min_1000',\n    'rolling_max_500_msignal',\n    'norm_1000_msignal',\n    'rolling_mean_10_msignal',\n    'abs_minbatch_slices2_msignal',\n    'rolling_min_50_msignal',\n    'rolling_min_500',\n    'rangebatch_slices2',\n    'abs_minbatch',\n    'meanbatch_slices2',\n    'abs_minbatch_msignal',\n    'minbatch_slices2_msignal',\n    'signal_shift_+2_msignal',\n    'signal_shift_-2_msignal',\n    'mean_abs_chgbatch_msignal',\n    'rolling_var_3_msignal',\n    'signal_shift_-2',\n    'signal_shift_+1',\n    'rolling_max_5000',\n    'rolling_var_5000',\n    'rolling_max_100',\n]    \n\nprint(len(BEST_FEATURES))\nFEATURES = [c for c in train.columns if c not in ['index, time', 'open_channels', 'group1', 'group2', 'index_msignal' ]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#from sklearn.decomposition import PCA\n\n#all_other_cols = [c for c in FEATURES if c not in BEST_FEATURES]\n\n#pca = PCA(n_components=1)\n#pca_train = pca.fit_transform(train[all_other_cols])\n#pca_test = pca.transform(test[all_other_cols])\n\n#train['pca_1'] = pca_train[:,0]\n#test['pca_1'] = pca_test[:,0]\n#BEST_FEATURES.append('pca_1')\n\n#del pca_train, pca_test, pca\n\n#y_train = train['open_channels']\n#train.drop(all_other_cols, inplace=True, axis=1)\n#test.drop(all_other_cols, inplace=True, axis=1)\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGB Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## reduce amount of data to speed things up\n\n#SAMPLE_RATE = 2\n\ny_train = train['open_channels']\nX = train[BEST_FEATURES]#[::SAMPLE_RATE]\ny = y_train#[::SAMPLE_RATE]\n\ngroups1 = train['group1']#[::SAMPLE_RATE]\ngroups2 = train['group2']#[::SAMPLE_RATE]\n\nX_test = test[BEST_FEATURES]\ngc.collect()\n\nprint(f'Original sizes: train: {train.shape}, y_train: {y_train.shape}' )\nprint(f'Original sizes: test: {test.shape}' )\nprint(f'Reduced train sizes: X: {X.shape}, y: {y.shape},  X_test: {X_test.shape}' )\nprint(f'Reduced test sizes: X_test: {X_test.shape}' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n#scaler = RobustScaler(copy=False)\n\n#scaler.fit(X[BEST_FEATURES].values)\n\n#X[BEST_FEATURES] = scaler.transform(X[BEST_FEATURES].to_numpy()) \n#X_test[BEST_FEATURES] = scaler.transform(X_test[BEST_FEATURES].to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nNUM_BOOST_ROUND = 2000 \nEARLY_STOPPING_ROUNDS = 100\nVERBOSE_EVAL = 100\nLEARNING_RATE = 0.05\nMAX_DEPTH = -1\nNUM_LEAVES = 250\n\nTOTAL_SPLITS = 6\n\nlgb_params2 = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'num_class': 11,\n    'metric': 'multi_logloss',\n    'learning_rate': 0.01987173774816051,\n    'lambda_l1': 0.00031963798315506463,\n    'lambda_l2': 0.18977456778807847,\n    'num_leaves': 171, \n    'feature_fraction': 0.58733782457345, \n    'bagging_fraction': 0.7057826081907392, \n    'bagging_freq': 4,\n    'seed': RANDOM_SEED,\n    #'max_depth': 73,\n    #'colsample_bytree': 0.6867118652742716,\n}\n\ndef run_for_group(groups):    \n    feat_imp_ = pd.DataFrame(np.zeros((len(BEST_FEATURES), 2)), columns=['vals','names'])\n    feat_imp_.names = BEST_FEATURES\n    oof_ = pd.DataFrame(np.zeros((len(X), 1))) \n    preds_ = pd.DataFrame(np.zeros((len(X_test), 1)))\n    kf = GroupKFold(n_splits=TOTAL_SPLITS)\n\n    fold = 1\n    for tr_idx, val_idx in kf.split(X, y, groups=groups):\n        print(f'====== Fold {fold:0.0f} of {TOTAL_SPLITS} ======')\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n\n        model = lgb.train(\n            lgb_params2, valid_names=[\"train\", \"valid\"], \n            train_set=lgb.Dataset(X_tr, y_tr ), # \n            num_boost_round = NUM_BOOST_ROUND,\n            valid_sets = [lgb.Dataset(X_val, y_val)], #  \n            verbose_eval = VERBOSE_EVAL,\n            early_stopping_rounds = EARLY_STOPPING_ROUNDS,\n            feval = multiclass_F1_Metric) ##MacroF1Metric\n\n        feat_imp_.vals += model.feature_importance() \n\n        oof = model.predict(X_val, num_iteration=model.best_iteration)\n        oof = np.argmax(oof, axis=1).reshape(-1)\n        test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n        test_preds = np.argmax(test_preds, axis=1).reshape(-1)\n\n        oof_.iloc[oof_.index.isin(val_idx)] = oof.reshape(-1,1)\n        preds_ += test_preds.reshape(-1,1)\n\n        print( f'OOF F1 score: {f1_score(y_val, oof, average = \"macro\")}') \n        print( f'OOF RMSE: {np.sqrt(mean_squared_error(y_val, oof))}') \n        fold = fold+1\n\n    return preds_ / TOTAL_SPLITS, oof_, feat_imp_\n\nyhat_group1, oof1, fi1 = run_for_group(groups1)\nyhat_group2, oof2, fi2 = run_for_group(groups2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"====== Fold 1 of 5 ======\nTraining until validation scores don't improve for 100 rounds\n[100]\ttrain's multi_logloss: 0.635213\ttrain's MacroF1Metric: 0.936415\n[200]\ttrain's multi_logloss: 0.286776\ttrain's MacroF1Metric: 0.936632\n[300]\ttrain's multi_logloss: 0.163175\ttrain's MacroF1Metric: 0.936808\n[400]\ttrain's multi_logloss: 0.11635\ttrain's MacroF1Metric: 0.936916\nEarly stopping, best iteration is:\n[327]\ttrain's multi_logloss: 0.145912\ttrain's MacroF1Metric: 0.936955"},{"metadata":{"trusted":true},"cell_type":"code","source":"fi1.sort_values(by='vals', inplace=True, ascending=False)\nfi1.head(33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi2.sort_values(by='vals', inplace=True, ascending=False)\nfi2.head(33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_lgb1 = f1_score(np.round(np.clip(oof1, 0, 10)).astype(int), y, average='macro')\nprint(f'OOF FULL F1MACRO: {f1_lgb1}')\nf1_lgb2 = f1_score(np.round(np.clip(oof2, 0, 10)).astype(int), y, average='macro')\nprint(f'OOF FULL F1MACRO: {f1_lgb2}')\n\ny_pred = np.round(np.clip((oof1+oof2)/2, 0, 10)).astype(int).values\noptR = OptimizedRounder()\noptR.fit(y_pred.reshape(-1,), y)\n\nopt_pred = optimize_predictions(y_pred, optR.coefficients()).astype(int)\nopt_f1 = f1_score(opt_pred, y, average='macro')\nprint(f'OPTIMIZED LGB F1MACRO: {opt_f1}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cm(y, opt_pred, 'LGB Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.round(np.clip((yhat_group1+yhat_group2)/2, 0, 10)).astype(int).values\nopt_pred = optimize_predictions(y_pred, optR.coefficients()).astype(int)\n\nsub = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv')\nsub['open_channels'] = np.round(np.clip(opt_pred, 0, 10)).astype(int)\nsub.to_csv('submission.csv', index=False, float_format='%.4f')\n\nsub.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}